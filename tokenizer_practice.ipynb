{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9991ea5",
   "metadata": {},
   "source": [
    "## BPE Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266f580",
   "metadata": {},
   "source": [
    "1. Identify frequent pairs - In each iteration scan the text to find the most commonly occuring pair of bytes(or characters.)\n",
    "2. Replace the pair with a new placeholder ID (one not already in use, eg., if we start with 0...255 the first place holder would be 256). Record this mapping in the lookup table. The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (for gpt-2, thats 50,257)\n",
    "3. Keep repeating steps 1 and 2, continually merging the most frequent pairs. Stop when no further compression is possible.\n",
    "4. To store the original text, reverse the process by substituting each ID with its corresponding pair, using the lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184677e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratham/Documents/Documents - Pratham’s MacBook Pro/tokenizers/tokenizers/.mark42/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "11: ,\n",
      "12: -\n",
      "13: .\n",
      "14: /\n",
      "15: 0\n",
      "16: 1\n",
      "17: 2\n",
      "18: 3\n",
      "19: 4\n",
      "20: 5\n",
      "21: 6\n",
      "22: 7\n",
      "23: 8\n",
      "24: 9\n",
      "25: :\n",
      "26: ;\n",
      "27: <\n",
      "28: =\n",
      "29: >\n",
      "30: ?\n",
      "31: @\n",
      "32: A\n",
      "33: B\n",
      "34: C\n",
      "35: D\n",
      "36: E\n",
      "37: F\n",
      "38: G\n",
      "39: H\n",
      "40: I\n",
      "41: J\n",
      "42: K\n",
      "43: L\n",
      "44: M\n",
      "45: N\n",
      "46: O\n",
      "47: P\n",
      "48: Q\n",
      "49: R\n",
      "50: S\n",
      "51: T\n",
      "52: U\n",
      "53: V\n",
      "54: W\n",
      "55: X\n",
      "56: Y\n",
      "57: Z\n",
      "58: [\n",
      "59: \\\n",
      "60: ]\n",
      "61: ^\n",
      "62: _\n",
      "63: `\n",
      "64: a\n",
      "65: b\n",
      "66: c\n",
      "67: d\n",
      "68: e\n",
      "69: f\n",
      "70: g\n",
      "71: h\n",
      "72: i\n",
      "73: j\n",
      "74: k\n",
      "75: l\n",
      "76: m\n",
      "77: n\n",
      "78: o\n",
      "79: p\n",
      "80: q\n",
      "81: r\n",
      "82: s\n",
      "83: t\n",
      "84: u\n",
      "85: v\n",
      "86: w\n",
      "87: x\n",
      "88: y\n",
      "89: z\n",
      "90: {\n",
      "91: |\n",
      "92: }\n",
      "93: ~\n",
      "94: �\n",
      "95: �\n",
      "96: �\n",
      "97: �\n",
      "98: �\n",
      "99: �\n",
      "100: �\n",
      "101: �\n",
      "102: �\n",
      "103: �\n",
      "104: �\n",
      "105: �\n",
      "106: �\n",
      "107: �\n",
      "108: �\n",
      "109: �\n",
      "110: �\n",
      "111: �\n",
      "112: �\n",
      "113: �\n",
      "114: �\n",
      "115: �\n",
      "116: �\n",
      "117: �\n",
      "118: �\n",
      "119: �\n",
      "120: �\n",
      "121: �\n",
      "122: �\n",
      "123: �\n",
      "124: �\n",
      "125: �\n",
      "126: �\n",
      "127: �\n",
      "128: �\n",
      "129: �\n",
      "130: �\n",
      "131: �\n",
      "132: �\n",
      "133: �\n",
      "134: �\n",
      "135: �\n",
      "136: �\n",
      "137: �\n",
      "138: �\n",
      "139: �\n",
      "140: �\n",
      "141: �\n",
      "142: �\n",
      "143: �\n",
      "144: �\n",
      "145: �\n",
      "146: �\n",
      "147: �\n",
      "148: �\n",
      "149: �\n",
      "150: �\n",
      "151: �\n",
      "152: �\n",
      "153: �\n",
      "154: �\n",
      "155: �\n",
      "156: �\n",
      "157: �\n",
      "158: �\n",
      "159: �\n",
      "160: �\n",
      "161: �\n",
      "162: �\n",
      "163: �\n",
      "164: �\n",
      "165: �\n",
      "166: �\n",
      "167: �\n",
      "168: �\n",
      "169: �\n",
      "170: �\n",
      "171: �\n",
      "172: �\n",
      "173: �\n",
      "174: �\n",
      "175: �\n",
      "176: �\n",
      "177: �\n",
      "178: �\n",
      "179: �\n",
      "180: �\n",
      "181: �\n",
      "182: �\n",
      "183: �\n",
      "184: �\n",
      "185: �\n",
      "186: �\n",
      "187: �\n",
      "188: \u0000\n",
      "189: \u0001\n",
      "190: \u0002\n",
      "191: \u0003\n",
      "192: \u0004\n",
      "193: \u0005\n",
      "194: \u0006\n",
      "195: \u0007\n",
      "196:\n",
      "197: \t\n",
      "198: \n",
      "\n",
      "199: \u000b\n",
      "200: \f\n",
      "201: \n",
      "202: \u000e\n",
      "203: \u000f\n",
      "204: \u0010\n",
      "205: \u0011\n",
      "206: \u0012\n",
      "207: \u0013\n",
      "208: \u0014\n",
      "209: \u0015\n",
      "210: \u0016\n",
      "211: \u0017\n",
      "212: \u0018\n",
      "213: \u0019\n",
      "214: \u001a\n",
      "215: \u001b\n",
      "216: \u001c\n",
      "217: \u001d\n",
      "218: \u001e\n",
      "219: \u001f\n",
      "220:  \n",
      "221: \n",
      "222: �\n",
      "223: �\n",
      "224: �\n",
      "225: �\n",
      "226: �\n",
      "227: �\n",
      "228: �\n",
      "229: �\n",
      "230: �\n",
      "231: �\n",
      "232: �\n",
      "233: �\n",
      "234: �\n",
      "235: �\n",
      "236: �\n",
      "237: �\n",
      "238: �\n",
      "239: �\n",
      "240: �\n",
      "241: �\n",
      "242: �\n",
      "243: �\n",
      "244: �\n",
      "245: �\n",
      "246: �\n",
      "247: �\n",
      "248: �\n",
      "249: �\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "266:  w\n",
      "267:  o\n",
      "268: en\n",
      "269:  c\n",
      "270: it\n",
      "271: is\n",
      "272: an\n",
      "273: or\n",
      "274: es\n",
      "275:  b\n",
      "276: ed\n",
      "277:  f\n",
      "278: ing\n",
      "279:  p\n",
      "280: ou\n",
      "281:  an\n",
      "282: al\n",
      "283: ar\n",
      "284:  to\n",
      "285:  m\n",
      "286:  of\n",
      "287:  in\n",
      "288:  d\n",
      "289:  h\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f0a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "11: ,\n",
      "12: -\n",
      "13: .\n",
      "14: /\n",
      "15: 0\n",
      "16: 1\n",
      "17: 2\n",
      "18: 3\n",
      "19: 4\n",
      "20: 5\n",
      "21: 6\n",
      "22: 7\n",
      "23: 8\n",
      "24: 9\n",
      "25: :\n",
      "26: ;\n",
      "27: <\n",
      "28: =\n",
      "29: >\n",
      "30: ?\n",
      "31: @\n",
      "32: A\n",
      "33: B\n",
      "34: C\n",
      "35: D\n",
      "36: E\n",
      "37: F\n",
      "38: G\n",
      "39: H\n",
      "40: I\n",
      "41: J\n",
      "42: K\n",
      "43: L\n",
      "44: M\n",
      "45: N\n",
      "46: O\n",
      "47: P\n",
      "48: Q\n",
      "49: R\n",
      "50: S\n",
      "51: T\n",
      "52: U\n",
      "53: V\n",
      "54: W\n",
      "55: X\n",
      "56: Y\n",
      "57: Z\n",
      "58: [\n",
      "59: \\\n",
      "60: ]\n",
      "61: ^\n",
      "62: _\n",
      "63: `\n",
      "64: a\n",
      "65: b\n",
      "66: c\n",
      "67: d\n",
      "68: e\n",
      "69: f\n",
      "70: g\n",
      "71: h\n",
      "72: i\n",
      "73: j\n",
      "74: k\n",
      "75: l\n",
      "76: m\n",
      "77: n\n",
      "78: o\n",
      "79: p\n",
      "80: q\n",
      "81: r\n",
      "82: s\n",
      "83: t\n",
      "84: u\n",
      "85: v\n",
      "86: w\n",
      "87: x\n",
      "88: y\n",
      "89: z\n",
      "90: {\n",
      "91: |\n",
      "92: }\n",
      "93: ~\n",
      "94: �\n",
      "95: �\n",
      "96: �\n",
      "97: �\n",
      "98: �\n",
      "99: �\n",
      "100: �\n",
      "101: �\n",
      "102: �\n",
      "103: �\n",
      "104: �\n",
      "105: �\n",
      "106: �\n",
      "107: �\n",
      "108: �\n",
      "109: �\n",
      "110: �\n",
      "111: �\n",
      "112: �\n",
      "113: �\n",
      "114: �\n",
      "115: �\n",
      "116: �\n",
      "117: �\n",
      "118: �\n",
      "119: �\n",
      "120: �\n",
      "121: �\n",
      "122: �\n",
      "123: �\n",
      "124: �\n",
      "125: �\n",
      "126: �\n",
      "127: �\n",
      "128: �\n",
      "129: �\n",
      "130: �\n",
      "131: �\n",
      "132: �\n",
      "133: �\n",
      "134: �\n",
      "135: �\n",
      "136: �\n",
      "137: �\n",
      "138: �\n",
      "139: �\n",
      "140: �\n",
      "141: �\n",
      "142: �\n",
      "143: �\n",
      "144: �\n",
      "145: �\n",
      "146: �\n",
      "147: �\n",
      "148: �\n",
      "149: �\n",
      "150: �\n",
      "151: �\n",
      "152: �\n",
      "153: �\n",
      "154: �\n",
      "155: �\n",
      "156: �\n",
      "157: �\n",
      "158: �\n",
      "159: �\n",
      "160: �\n",
      "161: �\n",
      "162: �\n",
      "163: �\n",
      "164: �\n",
      "165: �\n",
      "166: �\n",
      "167: �\n",
      "168: �\n",
      "169: �\n",
      "170: �\n",
      "171: �\n",
      "172: �\n",
      "173: �\n",
      "174: �\n",
      "175: �\n",
      "176: �\n",
      "177: �\n",
      "178: �\n",
      "179: �\n",
      "180: �\n",
      "181: �\n",
      "182: �\n",
      "183: �\n",
      "184: �\n",
      "185: �\n",
      "186: �\n",
      "187: �\n",
      "188: \u0000\n",
      "189: \u0001\n",
      "190: \u0002\n",
      "191: \u0003\n",
      "192: \u0004\n",
      "193: \u0005\n",
      "194: \u0006\n",
      "195: \u0007\n",
      "196:\n",
      "197: \t\n",
      "198: \n",
      "\n",
      "199: \u000b\n",
      "200: \f\n",
      "201: \n",
      "202: \u000e\n",
      "203: \u000f\n",
      "204: \u0010\n",
      "205: \u0011\n",
      "206: \u0012\n",
      "207: \u0013\n",
      "208: \u0014\n",
      "209: \u0015\n",
      "210: \u0016\n",
      "211: \u0017\n",
      "212: \u0018\n",
      "213: \u0019\n",
      "214: \u001a\n",
      "215: \u001b\n",
      "216: \u001c\n",
      "217: \u001d\n",
      "218: \u001e\n",
      "219: \u001f\n",
      "220:  \n",
      "221: \n",
      "222: �\n",
      "223: �\n",
      "224: �\n",
      "225: �\n",
      "226: �\n",
      "227: �\n",
      "228: �\n",
      "229: �\n",
      "230: �\n",
      "231: �\n",
      "232: �\n",
      "233: �\n",
      "234: �\n",
      "235: �\n",
      "236: �\n",
      "237: �\n",
      "238: �\n",
      "239: �\n",
      "240: �\n",
      "241: �\n",
      "242: �\n",
      "243: �\n",
      "244: �\n",
      "245: �\n",
      "246: �\n",
      "247: �\n",
      "248: �\n",
      "249: �\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "266:  w\n",
      "267:  o\n",
      "268: en\n",
      "269:  c\n",
      "270: it\n",
      "271: is\n",
      "272: an\n",
      "273: or\n",
      "274: es\n",
      "275:  b\n",
      "276: ed\n",
      "277:  f\n",
      "278: ing\n",
      "279:  p\n",
      "280: ou\n",
      "281:  an\n",
      "282: al\n",
      "283: ar\n",
      "284:  to\n",
      "285:  m\n",
      "286:  of\n",
      "287:  in\n",
      "288:  d\n",
      "289:  h\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "gpt3_tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "for i in range(300):\n",
    "    decoded = gpt3_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f2b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 Tokens: [9906, 4435]\n"
     ]
    }
   ],
   "source": [
    "enc_gpt4 = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(f\"GPT-4 Tokens: {enc_gpt4.encode('Hello World')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e761687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 Encoding: [15496, 2159]\n"
     ]
    }
   ],
   "source": [
    "enc_gpt3 = tiktoken.get_encoding(\"r50k_base\")\n",
    "print(f\"GPT-3 Encoding: {enc_gpt3.encode('Hello World')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Encoding: [15496, 995]\n"
     ]
    }
   ],
   "source": [
    "enc_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"GPT-2 Encoding: {enc_gpt2.encode('Hello world')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "548b3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Encoding: [15496, 995]\n"
     ]
    }
   ],
   "source": [
    "enc_gpt2 = tiktoken.get_encoding(\"gpt2\").encode('Hello world')\n",
    "print(f\"GPT-2 Encoding: {enc_gpt2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd626c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "dcd_gpt2 = tiktoken.get_encoding(\"gpt2\").decode([15496,995])\n",
    "print(f\"{dcd_gpt2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20b79078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Rico\n"
     ]
    }
   ],
   "source": [
    "dcd_gpt3 = tiktoken.get_encoding(\"r50k_base\").decode([15496,16707])\n",
    "print(f\"{dcd_gpt3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2553b2",
   "metadata": {},
   "source": [
    "## A Simple BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e600ea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "list_of_chars = [\"abc\",\"bad\",\"sad\",\"Ġ\",\"a\",\"b\"]\n",
    "if 'sad' in list_of_chars:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2a7582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'apple', 'chips', 'canes']\n",
      "[('banana', 2), ('apple', 3), ('chips', 4), ('canes', 11)]\n",
      "[2, 3, 4, 11]\n",
      "Key -> Value Mapping Down below - \n",
      "banana -> 2\n",
      "apple -> 3\n",
      "chips -> 4\n",
      "canes -> 11\n",
      "If you need to know how entries are present in your dictionary\n",
      "0 -> ('banana', 2)\n",
      "1 -> ('apple', 3)\n",
      "2 -> ('chips', 4)\n",
      "3 -> ('canes', 11)\n"
     ]
    }
   ],
   "source": [
    "inventory = {\n",
    "    'banana': 2,\n",
    "    'apple' : 3,\n",
    "    'chips' : 4,\n",
    "    'canes' : 11\n",
    "}\n",
    "inventory_keys = list(inventory.keys())\n",
    "print(inventory_keys)\n",
    "\n",
    "inventory_items = list(inventory.items())\n",
    "print(inventory_items)\n",
    "\n",
    "inventory_values = list(inventory.values())\n",
    "print(inventory_values)\n",
    "\n",
    "print(\"Key -> Value Mapping Down below - \")\n",
    "for key, value in inventory_items:\n",
    "    print(f\"{key} -> {value}\")\n",
    "\n",
    "print(\"If you need to know how entries are present in your dictionary\")\n",
    "for i, entry in enumerate(inventory.items()):\n",
    "    print(f\"{i} -> {entry}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee057ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ġspace\n"
     ]
    }
   ],
   "source": [
    "text = 'space'\n",
    "word = 'Ġ' + text\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fccac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # maps token_id to token_str (e.g., {11246:\"some\"})\n",
    "        self.vocab = {}\n",
    "        # maps token_str to token_id (eg., {\"some\":11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\" \n",
    "        Train the BPE tokenizer from scratch\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(Ġ)\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including 'Ġ' if present\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # Extend unique_chars with characters from processed_text that are not already included\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "        # Optionally, ensure Ġ is included if it is relevant to your text processing\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # Now create the vocab and inverse vocab dictionaries\n",
    "\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char : i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        #BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode = \"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0,p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to encode\n",
    "\n",
    "        Returns:\n",
    "            List (int): The list of token IDs.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        #Split text into tokens, keeping new lines intact\n",
    "        words = text.replace(\"\\n\", \"\\n \").split()    #Ensure '\\n' is treated a separate token\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\"+word) # Add 'Ġ' to words that follow a space or newline\n",
    "            else:\n",
    "                tokens.append(word) # Handle first word or standalone '/n' \n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token is contained in the vocabulary as is\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # Attempt to handle subword tokenization via BPE\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "    \n",
    "    \n",
    "    def tokenize_with_bpe(self, token):  # TODO fix this\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE Merges\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Return:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters  (as initial token ids)\n",
    "\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        # to be continued!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token ids back to a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token ids to decode.\n",
    "\n",
    "        Return:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "\n",
    "        decoded_string = \"\"\n",
    "\n",
    "        for token_id in token_ids: # ? [x] Fix me \n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"The token id: {token_id} is not found.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                # replace the Ġ with space\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "    \n",
    "\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None) # the .get(token, None) - searches for the token in dict, if not there will return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode = \"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mark42 (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
